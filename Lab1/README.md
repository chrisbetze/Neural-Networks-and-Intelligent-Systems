# Supervised Learning in UCI Machine Learning Repository

## Dataset 1: Japanese Credit Screening Data Set
### Information
Examples represent positive and negative instances of people who were and were not granted credit. 
The theory was generated by talking to the individuals at a Japanese company that grants credit.
Dataset consists of samples 45% of first class (granted credit) and 55% of second (not granted credit).
### Preprocessing
* Missing values: We don't ignore columns with missing values. Instead in numeric data we apply mean strategy and in string data, most frequent strategy.
* Non ordinal features: Convert non ordinal and categorical features to binary vectors.
### Classification
We compare two scikit-learn classifiers: Gaussian Naive Bayes, kNN
### Optimization
Fine tuning using Pipeline, GridSearch and Cross-validation techniques. Pipeline steps are PCA, Variance Threshold Selector, Standard Scaler and Random Over Sampler.
### Results
We achieve an accuracy of 92% using kNN. 
## Dataset 2: Statlog (Landsat Satellite) Data Set
### Information
The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. 
The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. 
The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. 
Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. 
The number is a code for the following classes:
1) red soil
2) cotton crop
3) grey soil
4) damp grey soil
5) soil with vegetation stubble
6) mixture class (all types present)
7) very damp grey soil

NB. There are no examples with class 6 in this dataset.
### Preprocessing
No missing values and all features are numeric.
### Classification
We compare different scikit-learn classifiers: Gaussian Naive Bayes, kNN, Multi-Layer Perceptron (MLP), Support Vector Machines (SVM)
### Optimization
Fine tuning using Pipeline, GridSearch and Cross-validation techniques. Pipeline steps are PCA, Variance Threshold Selector, Standard Scaler and Random Over Sampler.
### Results
We achieve an accuracy of 90% using MLP.
